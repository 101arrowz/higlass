{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "\n",
    "import os.path as op\n",
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LrhSsEkxUy3acAwGdV5sVg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shortuuid\n",
    "shortuuid.uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembly = 'hg19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = op.expanduser(\"~/data\")\n",
    "output_dir = op.join(data_dir, assembly)   # where all of the intermediate output will be stored\n",
    "base_ucsc_dir = op.join(data_dir, 'ucsc-data/{}'.format(assembly))  # where all of the files downloaded from UCSC will be stored\n",
    "\n",
    "import shutil\n",
    "\n",
    "# create a directory to store intermediate output files\n",
    "def get_outfile(table_name):\n",
    "    outfile = op.join(output_dir, 'genbank-output/{}'.format(table_name))\n",
    "    if op.exists(outfile):\n",
    "        shutil.rmtree(outfile)\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the chromosome lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 249250621 492449994\n"
     ]
    }
   ],
   "source": [
    "def get_chrom_lengths(base_dir):\n",
    "    '''\n",
    "    Get the cumulative start positions for the chromosomes in an assembly. The chromosomes\n",
    "    will be sorted alphabetically by their names.\n",
    "    \n",
    "    :param base_dir: A directory containing meta data about a genome assembly\n",
    "    :return: A dictionary of the from { 'chr2': 234323432 }, showing at which position\n",
    "             chromosomes start.\n",
    "    '''\n",
    "    chromLengths = (sc.textFile(op.join(base_dir, 'chromInfo.txt.gz'))\n",
    "                    .map(lambda x: x.split('\\t'))\n",
    "                    .map(lambda x: {'chrom': x[0], 'length': int(x[1]) })\n",
    "                    .collect())\n",
    "    \n",
    "    cum_chrom_lengths = {}\n",
    "    curr_cum_lengths = 0\n",
    "    \n",
    "    for x in sorted(chromLengths, key=lambda x: -x['length']):\n",
    "        cum_chrom_lengths[x['chrom']] = curr_cum_lengths\n",
    "        curr_cum_lengths += x['length']\n",
    "        \n",
    "    return cum_chrom_lengths\n",
    "\n",
    "cum_chrom_lengths = get_chrom_lengths(base_ucsc_dir)\n",
    "\n",
    "print cum_chrom_lengths['chr1'], cum_chrom_lengths['chr2'], cum_chrom_lengths['chr3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the refgene data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'exonEnds': u'15469389,15471514,15473730,15476045,15478082,15484120', 'geneName': u'EAF1', 'chromOffset': 492449994, 'name': u'NM_033083', 'txStart': u'15469063', 'exonCount': u'6', 'strand': u'+', 'cdsEnd': u'15480662', 'genomeTxStart': 507919057, 'geneLength': 15057, 'cdsStart': u'15469286', 'chrom': u'chr3', 'genomeTxEnd': 507934114, 'txEnd': u'15484120', 'exonStarts': u'15469063,15471419,15473593,15475854,15477848,15480615'}]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "def parse_exon_positions(exon_positions_str):\n",
    "    return map(int, exon_positions_str.strip(\",\").split(','))\n",
    "\n",
    "def load_refgene_data(base_dir):\n",
    "    '''\n",
    "    Load the UCSC refgene data for a particular assembly.\n",
    "    \n",
    "    :param base_dir: The directory which contains the refGene.txt.gz file.\n",
    "    '''\n",
    "    refGene = (sc.textFile(op.join(base_dir, 'refGene.txt.gz'))\n",
    "               .map(lambda x: x.split('\\t'))\n",
    "               .map(lambda x: {'name': x[1],\n",
    "                               'chrom': x[2],\n",
    "                               'strand': x[3],\n",
    "                               'txStart': x[4],\n",
    "                               'txEnd': x[5],\n",
    "                               'cdsStart': x[6],\n",
    "                               'cdsEnd': x[7],\n",
    "                               'exonCount': x[8],\n",
    "                               'exonStarts': x[9].strip(','),\n",
    "                               'exonEnds': x[10].strip(','),\n",
    "                               'chromOffset': cum_chrom_lengths[x[2]],\n",
    "                               'genomeTxStart': cum_chrom_lengths[x[2]] + int(x[4]),\n",
    "                               'genomeTxEnd': cum_chrom_lengths[x[2]] + int(x[5]),\n",
    "                               'geneName': x[12],\n",
    "                               'geneLength': int(x[5]) - int(x[4]),\n",
    "                               })\n",
    "            )\n",
    "    \n",
    "    return refGene\n",
    "\n",
    "refGene = load_refgene_data(base_ucsc_dir)\n",
    "### add the genomic position\n",
    "print refGene.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the references\n",
    "\n",
    "For this exercise, the result should be a tsv with the following columns:\n",
    "\n",
    "```\n",
    "<taxId> <geneId> <refSeqID> <citation count>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((9, 1246500), {'count': 1})]\n",
      "[((956149, 14564489), {'count': 1})]\n"
     ]
    }
   ],
   "source": [
    "def load_gene_counts(genbank_dir):\n",
    "    gene2pubmed = (sc.textFile(op.join(genbank_dir, \"gene2pubmed\"))\n",
    "                     .filter(lambda x: x[0] != '#')\n",
    "                     .map(lambda x: x.split('\\t'))\n",
    "                     .map(lambda p: {'taxid': int(p[0]), 'geneid': int(p[1]), 'pmid': int(p[2]), 'count': 1})\n",
    "                     .map(lambda x: ((x['taxid'], x['geneid']), {'count': x['count']}))\n",
    "                     )\n",
    "    \n",
    "    def reduce_count(r1, r2):\n",
    "        '''\n",
    "        A reduce function that simply counts the number of elements in the table.\n",
    "        \n",
    "        @param r1: A Row\n",
    "        @param r2: A Row\n",
    "        @return: A new Row, equal to the first Row with a summed count.\n",
    "        '''\n",
    "        #print >>sys.stderr, \"r1:\", r1\n",
    "        r1['count'] += r2['count']\n",
    "        return r1\n",
    "\n",
    "    print gene2pubmed.take(1)\n",
    "    reduced_gene2pubmed = gene2pubmed.reduceByKey(reduce_count)\n",
    "    \n",
    "    outfile = get_outfile('taxid-geneid-count')\n",
    "\n",
    "    (reduced_gene2pubmed\n",
    "        .map(lambda x: \"{}\\t{}\\t{}\".format(x[0][0], x[0][1], x[1]['count']))\n",
    "        .saveAsTextFile(outfile)\n",
    "        )\n",
    "    \n",
    "    return reduced_gene2pubmed\n",
    "\n",
    "\n",
    "taxid_geneid_count = load_gene_counts(op.join(data_dir, 'genbank-data/'))\n",
    "print taxid_geneid_count.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the gene2refseq annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'-', (9, 1246500)), (u'-', (9, 1246501)), (u'-', (9, 1246502)), (u'-', (9, 1246503)), (u'-', (9, 1246504)), (u'-', (9, 1246505)), (u'-', (9, 1246509)), (u'-', (9, 1246510)), (u'-', (9, 3722426)), (u'-', (9, 8655732))]\n",
      "[(u'XM_009054708', (225164, 20243866))]\n"
     ]
    }
   ],
   "source": [
    "def take_one(r1, r2):\n",
    "    return r1\n",
    "\n",
    "def load_refseq2gene(genbank_base_dir):\n",
    "    '''\n",
    "    Get the mapping from refseq IDs to gene IDs\n",
    "    \n",
    "    :param genbank_base_dir: The directory that contains all of the genbank files.\n",
    "    :return: A set of tuples of the form (refseq_id, (taxid, geneid))\n",
    "    '''\n",
    "    gene2refseq = (sc.textFile(op.join(genbank_base_dir, 'gene2refseq'))\n",
    "                   .filter(lambda x: x[0] != '#')\n",
    "                   .map(lambda x: x.split('\\t'))\n",
    "                   .map(lambda p: {'taxid': int(p[0]), 'geneid': int(p[1]), 'refseqid': p[3] })\n",
    "                   .map(lambda x: (x['refseqid'].split('.')[0], (x['taxid'], x['geneid'])))\n",
    "                   )\n",
    "    \n",
    "    def reduce_by_refseq_id(r1, r2):\n",
    "        # because we're just looking for a mapping from geneId to refseqId, we just need to throw\n",
    "        # away single entries with identical refseq ids\n",
    "        return r1\n",
    "    \n",
    "    print gene2refseq.take(10)\n",
    "    refseq2gene = gene2refseq.reduceByKey(take_one)\n",
    "    print refseq2gene.take(1)\n",
    "    \n",
    "    outfile = get_outfile('refseqid-taxid-geneid')\n",
    "\n",
    "    (refseq2gene.map(lambda x: \"{}\\t{}\\t{}\".format(x[0], x[1][0], x[1][1]))\n",
    "         .saveAsTextFile(outfile)\n",
    "    )\n",
    "    return refseq2gene\n",
    "\n",
    "refseqid_taxid_geneid = load_refseq2gene(op.join(data_dir, 'genbank-data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the reference counts and refseq to geneid translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'XM_009054708', (225164, 20243866))]\n"
     ]
    }
   ],
   "source": [
    "print refseqid_taxid_geneid.take(1)\n",
    "\n",
    "def take_one(r1, r2):\n",
    "    return r1\n",
    "\n",
    "def take_max(r1, r2):\n",
    "    if r1 > r2:\n",
    "        return r1\n",
    "    else:\n",
    "        return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count1: 9519075\n",
      "count2: 7364241\n",
      "[((4577, 103654904), u'XR_566429')]\n",
      "taxid_geneid_count [((956149, 14564489), {'count': 1})]\n",
      "1. taxid_geneid_count.count(): 8516870\n",
      "2. taxid_geneid_count.count(): 8516870\n",
      "[((6279, 6095747), ({'count': 1}, u'XM_001892259'))]\n",
      "[((703352, 12152382), {'count': 1})]\n"
     ]
    }
   ],
   "source": [
    "def join_counts_and_ids(refseqid_taxid_geneid, taxid_geneid_count):\n",
    "    taxid_geneid_refseq = refseqid_taxid_geneid.map(lambda x: (x[1], x[0]))\n",
    "    print \"count1:\", taxid_geneid_refseq.count()\n",
    "    taxid_geneid_refseq = taxid_geneid_refseq.reduceByKey(take_one)\n",
    "    print \"count2:\", taxid_geneid_refseq.count()\n",
    "    \n",
    "    \n",
    "    '''    \n",
    "    taxid_geneid_refseq = (sc.textFile(op.join(output_dir, 'genbank-output/refseqid-taxid-geneid'))\n",
    "                   .map(lambda x: x.split())\n",
    "                   .map(lambda x: ((int(x[1]), int(x[2])), x[0]))\n",
    "                        )\n",
    "    '''\n",
    "    print taxid_geneid_refseq.take(1)\n",
    "    \n",
    "    '''\n",
    "    (sc.textFile(op.join(output_dir, 'genbank-output/taxid-geneid-count'))\n",
    "                          .map(lambda x: x.split())\n",
    "                          .map(lambda x: ((int(x[0]), int(x[1])), int(x[2])))\n",
    "                          )\n",
    "    '''\n",
    "    print \"taxid_geneid_count\", taxid_geneid_count.take(1)\n",
    "    print \"1. taxid_geneid_count.count():\", taxid_geneid_count.count()\n",
    "    taxid_geneid_count = taxid_geneid_count.reduceByKey(take_max)\n",
    "    print \"2. taxid_geneid_count.count():\", taxid_geneid_count.count()\n",
    "    \n",
    "    taxid_geneid_count_refseq = taxid_geneid_count.join(taxid_geneid_refseq)\n",
    "    print taxid_geneid_count_refseq.take(1)\n",
    "    print taxid_geneid_count.take(1)\n",
    "    \n",
    "    outfile = get_outfile('taxid-geneid-refseqid-count')\n",
    "\n",
    "    (taxid_geneid_count_refseq.map(lambda x: \"{}\\t{}\\t{}\\t{}\".format(x[0][0],\n",
    "                                                                  x[0][1],\n",
    "                                                                  x[1][1],\n",
    "                                                                  x[1][0]))\n",
    "     .saveAsTextFile(outfile)\n",
    "     )\n",
    "    return taxid_geneid_count_refseq\n",
    "\n",
    "taxid_geneid_count_refseq = join_counts_and_ids(refseqid_taxid_geneid, taxid_geneid_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the refgene data with the count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'NM_033083', {'exonEnds': u'15469389,15471514,15473730,15476045,15478082,15484120', 'geneName': u'EAF1', 'chromOffset': 492449994, 'name': u'NM_033083', 'txStart': u'15469063', 'exonCount': u'6', 'strand': u'+', 'cdsEnd': u'15480662', 'genomeTxStart': 507919057, 'geneLength': 15057, 'cdsStart': u'15469286', 'chrom': u'chr3', 'genomeTxEnd': 507934114, 'txEnd': u'15484120', 'exonStarts': u'15469063,15471419,15473593,15475854,15477848,15480615'})]\n",
      "[(u'XM_001892259', {'count': 1})]\n",
      "[(u'NM_001466', ({'exonEnds': u'42638630', 'geneName': u'FZD2', 'chromOffset': 2655442424, 'name': u'NM_001466', 'txStart': u'42634811', 'exonCount': u'1', 'strand': u'+', 'cdsEnd': u'42636754', 'genomeTxStart': 2698077235, 'exonStarts': u'42634811', 'cdsStart': u'42635056', 'chrom': u'chr17', 'genomeTxEnd': 2698081054, 'txEnd': u'42638630', 'geneLength': 3819}, {'count': 23}))]\n"
     ]
    }
   ],
   "source": [
    "def join_refgene_and_counts(refGene, taxid_geneid_count_refseq):\n",
    "    '''\n",
    "    Combine the refGene information about the genes with the citation\n",
    "    count information.\n",
    "    '''\n",
    "    refseqid_refgene = refGene.map(lambda x: (x['name'], x))\n",
    "    \n",
    "    print refseqid_refgene.take(1)\n",
    "    \n",
    "    refseqid_count = taxid_geneid_count_refseq.map(lambda x: (x[1][1], x[1][0]))\n",
    "    \n",
    "    print refseqid_count.take(1)\n",
    "    \n",
    "    refseqid_refgene_count = refseqid_refgene.join(refseqid_count)\n",
    "\n",
    "    print refseqid_refgene_count.take(1)\n",
    "    \n",
    "\n",
    "    return refseqid_refgene_count\n",
    "\n",
    "refseqid_refgene_count = join_refgene_and_counts(refGene, taxid_geneid_count_refseq)\n",
    "\n",
    "outfile = get_outfile('refgene-count')\n",
    "(refseqid_refgene_count.map(lambda x: \"{name}\\t{chrom}\\t{strand}\\t{txStart}\\t{txEnd}\\t{genomeTxStart}\\t{genomeTxEnd}\\t{cdsStart}\\t{cdsEnd}\\t{exonCount}\\t{exonStarts}\\t{exonEnds}\\t{geneName}\\t{count}\\t{uid}\"\n",
    "                            .format(count=x[1][1]['count'],uid=shortuuid.uuid(), **x[1][0]))\n",
    " .saveAsTextFile(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'NM_001466', ({'exonEnds': u'42638630', 'geneName': u'FZD2', 'chromOffset': 2655442424, 'name': u'NM_001466', 'txStart': u'42634811', 'exonCount': u'1', 'strand': u'+', 'cdsEnd': u'42636754', 'genomeTxStart': 2698077235, 'exonStarts': u'42634811', 'cdsStart': u'42635056', 'chrom': u'chr17', 'genomeTxEnd': 2698081054, 'txEnd': u'42638630', 'geneLength': 3819}, {'count': 23}))]\n"
     ]
    }
   ],
   "source": [
    "print refseqid_refgene_count.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "refseqid_refgene_count_plus = refseqid_refgene_count.filter(lambda x: x[1][0]['strand'] == '+')\n",
    "\n",
    "outfile = get_outfile('refgene-count-plus')\n",
    "(refseqid_refgene_count_plus.map(lambda x: \"{name}\\t{chrom}\\t{strand}\\t{txStart}\\t{txEnd}\\t{genomeTxStart}\\t{genomeTxEnd}\\t{cdsStart}\\t{cdsEnd}\\t{exonCount}\\t{exonStarts}\\t{exonEnds}\\t{geneName}\\t{count}\\t{uid}\"\n",
    "                            .format(count=x[1][1]['count'],uid=shortuuid.uuid(), **x[1][0]))\n",
    " .saveAsTextFile(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "refseqid_refgene_count_minus = refseqid_refgene_count.filter(lambda x: x[1][0]['strand'] == '-')\n",
    "\n",
    "outfile = get_outfile('refgene-count-minus')\n",
    "(refseqid_refgene_count_minus.map(lambda x: \"{name}\\t{chrom}\\t{strand}\\t{txStart}\\t{txEnd}\\t{genomeTxStart}\\t{genomeTxEnd}\\t{cdsStart}\\t{cdsEnd}\\t{exonCount}\\t{exonStarts}\\t{exonEnds}\\t{geneName}\\t{count}\\t{uid}\"\n",
    "                            .format(count=x[1][1]['count'],uid=shortuuid.uuid(), **x[1][0]))\n",
    " .saveAsTextFile(outfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((9, 1246500), {'count': 1})]\n",
      "[(u'-', (9, 1246500)), (u'-', (9, 1246501)), (u'-', (9, 1246502)), (u'-', (9, 1246503)), (u'-', (9, 1246504)), (u'-', (9, 1246505)), (u'-', (9, 1246509)), (u'-', (9, 1246510)), (u'-', (9, 3722426)), (u'-', (9, 8655732))]\n",
      "[(u'XM_009103278', (3711, 103827740))]\n",
      "[((81985, 17878053), u'XM_006284442')]\n",
      "[((670483, 19307480), ({'count': 1}, u'XM_007869167'))]\n",
      "[((8030, 100195418), {'count': 1})]\n",
      "[(u'NM_033083', {'exonEnds': u'15469389,15471514,15473730,15476045,15478082,15484120', 'geneName': u'EAF1', 'chromOffset': 492449994, 'name': u'NM_033083', 'txStart': u'15469063', 'exonCount': u'6', 'strand': u'+', 'cdsEnd': u'15480662', 'genomeTxStart': 507919057, 'geneLength': 15057, 'cdsStart': u'15469286', 'chrom': u'chr3', 'genomeTxEnd': 507934114, 'txEnd': u'15484120', 'exonStarts': u'15469063,15471419,15473593,15475854,15477848,15480615'})]\n",
      "[(u'XM_007595264', {'count': 1})]\n",
      "[(u'NM_004265', ({'exonEnds': u'61596069,61605360,61608003,61608197,61615756,61624543,61625002,61630541,61630850,61631258,61632749,61634826', 'geneName': u'FADS2', 'chromOffset': 1971178450, 'name': u'NM_004265', 'txStart': u'61595504', 'exonCount': u'12', 'strand': u'+', 'cdsEnd': u'61633161', 'genomeTxStart': 2032773954, 'geneLength': 39322, 'cdsStart': u'61595862', 'chrom': u'chr11', 'genomeTxEnd': 2032813276, 'txEnd': u'61634826', 'exonStarts': u'61595504,61605249,61607805,61608095,61615630,61624482,61624925,61630443,61630753,61631178,61632623,61633109'}, {'count': 113}))]\n"
     ]
    }
   ],
   "source": [
    "assembly = 'hg19'\n",
    "output_dir = op.join(data_dir, assembly)    # where all of the intermediate output will be stored\n",
    "base_ucsc_dir = op.join(data_dir, 'ucsc-data/{}'.format(assembly))  # where all of the files downloaded from UCSC will be stored\n",
    "\n",
    "cum_chrom_lengths = get_chrom_lengths(base_ucsc_dir)\n",
    "refGene = load_refgene_data(base_ucsc_dir)\n",
    "taxid_geneid_count = load_gene_counts(op.join(data_dir, 'genbank-data/'))\n",
    "refseqid_taxid_geneid = load_refseq2gene(op.join(data_dir, 'genbank-data'))\n",
    "taxid_geneid_count_refseq = join_counts_and_ids(refseqid_taxid_geneid, taxid_geneid_count)\n",
    "refseqid_refgene_count = join_refgene_and_counts(refGene, taxid_geneid_count_refseq)\n",
    "\n",
    "outfile = get_outfile('refgene-count')\n",
    "(refseqid_refgene_count.map(lambda x: \"{name}\\t{chrom}\\t{strand}\\t{txStart}\\t{txEnd}\\t{genomeTxStart}\\t{genomeTxEnd}\\t{cdsStart}\\t{cdsEnd}\\t{exonCount}\\t{exonStarts}\\t{exonEnds}\\t{geneName}\\t{count}\\t{uid}\"\n",
    "                            .format(count=x[1][1]['count'],uid=shortuuid.uuid(), **x[1][0]))\n",
    " .saveAsTextFile(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fb753e2cd48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0massembly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hg19'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massembly\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# where all of the intermediate output will be stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbase_ucsc_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ucsc-data/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massembly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# where all of the files downloaded from UCSC will be stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'op' is not defined"
     ]
    }
   ],
   "source": [
    "assembly = 'hg19'\n",
    "output_dir = op.join(data_dir, assembly)    # where all of the intermediate output will be stored\n",
    "base_ucsc_dir = op.join(data_dir, 'ucsc-data/{}'.format(assembly))  # where all of the files downloaded from UCSC will be stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o650.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 134.0 failed 1 times, most recent failure: Lost task 6.0 in stage 134.0 (TID 1695, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1494, in func\n    for x in iterator:\n  File \"<ipython-input-28-54f3fadcd57a>\", line 1, in <lambda>\nTypeError: tuple indices must be integers, not str\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1494, in func\n    for x in iterator:\n  File \"<ipython-input-28-54f3fadcd57a>\", line 1, in <lambda>\nTypeError: tuple indices must be integers, not str\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-54f3fadcd57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m (refseqid_refgene_count_plus.map(lambda x: \"{name}\\t{chrom}\\t{strand}\\t{txStart}\\t{txEnd}\\t{genomeTxStart}\\t{genomeTxEnd}\\t{cdsStart}\\t{cdsEnd}\\t{exonCount}\\t{exonStarts}\\t{exonEnds}\\t{geneName}\\t{count}\\t{uid}\"\n\u001b[1;32m      5\u001b[0m                             .format(count=x[1][1]['count'],uid=shortuuid.uuid(), **x[1][0]))\n\u001b[0;32m----> 6\u001b[0;31m  .saveAsTextFile(outfile))\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o650.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 134.0 failed 1 times, most recent failure: Lost task 6.0 in stage 134.0 (TID 1695, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1494, in func\n    for x in iterator:\n  File \"<ipython-input-28-54f3fadcd57a>\", line 1, in <lambda>\nTypeError: tuple indices must be integers, not str\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 133, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1494, in func\n    for x in iterator:\n  File \"<ipython-input-28-54f3fadcd57a>\", line 1, in <lambda>\nTypeError: tuple indices must be integers, not str\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "refseqid_refgene_count_plus = refseqid_refgene_count.filter(lambda x: x['chrom'] == '+')\n",
    "\n",
    "outfile = get_outfile('refgene-count-plus')\n",
    "(refseqid_refgene_count_plus.map(lambda x: \"{name}\\t{chrom}\\t{strand}\\t{txStart}\\t{txEnd}\\t{genomeTxStart}\\t{genomeTxEnd}\\t{cdsStart}\\t{cdsEnd}\\t{exonCount}\\t{exonStarts}\\t{exonEnds}\\t{geneName}\\t{count}\\t{uid}\"\n",
    "                            .format(count=x[1][1]['count'],uid=shortuuid.uuid(), **x[1][0]))\n",
    " .saveAsTextFile(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
